{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Inference Optimizer: End-to-End Demonstration\n",
    "\n",
    "This notebook demonstrates the complete optimization pipeline for transformer-based generative models using the ML Inference Optimizer framework. We'll walk through:\n",
    "\n",
    "1. Loading a pre-trained model (GPT-2)\n",
    "2. Profiling the baseline performance\n",
    "3. Analyzing bottlenecks\n",
    "4. Applying optimizations step-by-step\n",
    "5. Measuring the performance improvements\n",
    "6. Visualizing the results\n",
    "\n",
    "Throughout the process, we'll show how the system automatically detects bottlenecks and applies the most effective optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Optional, Tuple, Union, Any\n",
    "\n",
    "# Import our optimization toolkit components\n",
    "from baseline.model_loader import load_model, HuggingFaceModelLoader\n",
    "from profiling.torch_profiler import ProfileManager\n",
    "from profiling.bottleneck_analyzer import BottleneckAnalyzer, BottleneckType\n",
    "from profiling.kernel_profiler import KernelProfiler\n",
    "from profiling.memory_tracker import MemoryTracker\n",
    "from parallelism.auto_config import AutoParallelConfig\n",
    "from kernels.attention.flash_attention import FlashAttention3, FlashAttentionConfig, ModelConverter\n",
    "from utils.visualization_utils import plot_performance_comparison, plot_memory_usage, plot_latency_breakdown\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading a Pre-trained Model\n",
    "\n",
    "We'll start by loading a pre-trained GPT-2 model from Hugging Face. This model will serve as our baseline for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a model to optimize (smaller for demonstration purposes)\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "# Create model loader\n",
    "loader = HuggingFaceModelLoader(device=device, dtype=torch.float16)\n",
    "\n",
    "# Load the model\n",
    "print(f\"Loading {model_name} model...\")\n",
    "model = loader.load_model(model_name)\n",
    "\n",
    "# Print model summary\n",
    "model_config = loader.get_model_config()\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Number of parameters: {model.num_parameters / 1e6:.2f}M\")\n",
    "print(f\"Hidden size: {model_config['hidden_size']}\")\n",
    "print(f\"Number of layers: {model_config['num_hidden_layers']}\")\n",
    "print(f\"Number of attention heads: {model_config['num_attention_heads']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Sample Input\n",
    "\n",
    "We'll create sample input for running inference with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input parameters\n",
    "batch_size = 8\n",
    "seq_len = 512\n",
    "\n",
    "# Generate sample input\n",
    "sample_input = loader.get_sample_input(batch_size, seq_len)\n",
    "print(f\"Sample input shape: {sample_input['input_ids'].shape}\")\n",
    "\n",
    "# Create some actual text inputs for qualitative testing\n",
    "tokenizer = loader.tokenizer\n",
    "text_inputs = [\n",
    "    \"The key to artificial intelligence has always been\",\n",
    "    \"In the distant future, humanity will\",\n",
    "    \"The relationship between technology and society is\",\n",
    "    \"When we think about the ethical implications of AI, we must consider\"\n",
    "]\n",
    "\n",
    "# Tokenize inputs\n",
    "text_tokens = tokenizer(text_inputs, padding=True, return_tensors=\"pt\").to(device)\n",
    "print(f\"Text input shape: {text_tokens['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Measuring Baseline Performance\n",
    "\n",
    "Before applying any optimizations, we'll measure the baseline performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(model, inputs, num_runs=5, warmup_runs=2):\n",
    "    \"\"\"Benchmark inference performance.\"\"\"\n",
    "    # Set to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Perform warmup runs\n",
    "    for _ in range(warmup_runs):\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "    \n",
    "    # Measure performance\n",
    "    latencies = []\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        latencies.append((time.time() - start_time) * 1000)  # Convert to ms\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_latency = sum(latencies) / len(latencies)\n",
    "    min_latency = min(latencies)\n",
    "    max_latency = max(latencies)\n",
    "    p95_latency = sorted(latencies)[int(len(latencies) * 0.95)]\n",
    "    \n",
    "    # Calculate throughput (samples/second)\n",
    "    throughput = (batch_size * 1000) / avg_latency\n",
    "    \n",
    "    return {\n",
    "        \"avg_latency_ms\": avg_latency,\n",
    "        \"min_latency_ms\": min_latency,\n",
    "        \"max_latency_ms\": max_latency,\n",
    "        \"p95_latency_ms\": p95_latency,\n",
    "        \"throughput\": throughput,\n",
    "        \"latencies\": latencies\n",
    "    }\n",
    "\n",
    "# Measure baseline performance\n",
    "print(\"Measuring baseline performance...\")\n",
    "baseline_perf = benchmark_inference(model, sample_input)\n",
    "\n",
    "print(f\"Baseline Average Latency: {baseline_perf['avg_latency_ms']:.2f} ms\")\n",
    "print(f\"Baseline Throughput: {baseline_perf['throughput']:.2f} samples/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation Performance\n",
    "\n",
    "Let's also measure the text generation performance, which is more representative of real-world usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_generation(model, inputs, max_new_tokens=30, num_runs=3):\n",
    "    \"\"\"Benchmark text generation performance.\"\"\"\n",
    "    # Set to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Measure performance\n",
    "    latencies = []\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        latencies.append((time.time() - start_time) * 1000)  # Convert to ms\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_latency = sum(latencies) / len(latencies)\n",
    "    tokens_per_second = (inputs[\"input_ids\"].shape[0] * max_new_tokens * 1000) / avg_latency\n",
    "    \n",
    "    return {\n",
    "        \"avg_latency_ms\": avg_latency,\n",
    "        \"tokens_per_second\": tokens_per_second,\n",
    "        \"generated_text\": outputs,\n",
    "        \"latencies\": latencies\n",
    "    }\n",
    "\n",
    "# Measure baseline generation performance\n",
    "print(\"Measuring baseline text generation performance...\")\n",
    "baseline_gen_perf = benchmark_generation(model, text_tokens)\n",
    "\n",
    "print(f\"Baseline Generation Latency: {baseline_gen_perf['avg_latency_ms']:.2f} ms\")\n",
    "print(f\"Baseline Tokens Per Second: {baseline_gen_perf['tokens_per_second']:.2f}\")\n",
    "\n",
    "# Display one generated output\n",
    "generated_text = tokenizer.decode(baseline_gen_perf['generated_text'][0], skip_special_tokens=True)\n",
    "print(f\"\\nSample generated text:\\n{generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Profiling and Bottleneck Analysis\n",
    "\n",
    "Now, we'll profile the model to identify performance bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create profilers\n",
    "profile_manager = ProfileManager()\n",
    "kernel_profiler = KernelProfiler()\n",
    "memory_tracker = MemoryTracker()\n",
    "\n",
    "# Start profiling\n",
    "print(\"Starting model profiling...\")\n",
    "profile_manager.start()\n",
    "kernel_profiler.start()\n",
    "memory_tracker.start()\n",
    "\n",
    "# Run model with profiling active\n",
    "with torch.no_grad():\n",
    "    _ = model(**sample_input)\n",
    "\n",
    "# Stop profiling\n",
    "memory_stats = memory_tracker.stop()\n",
    "kernel_results = kernel_profiler.stop()\n",
    "profile_results = profile_manager.stop()\n",
    "\n",
    "print(\"Profiling complete\")\n",
    "\n",
    "# Analyze bottlenecks\n",
    "bottleneck_analyzer = BottleneckAnalyzer(profile_results, kernel_results)\n",
    "bottleneck_report = bottleneck_analyzer.analyze()\n",
    "\n",
    "# Print bottleneck report\n",
    "print(bottleneck_report.format_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot memory usage over time\n",
    "memory_tracker.plot_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Operation Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot operation time breakdown\n",
    "profile_results.plot_operator_pie_chart()\n",
    "profile_results.plot_kernel_pie_chart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Automatic Optimization Pipeline\n",
    "\n",
    "Now we'll apply our optimization pipeline to the model based on the bottleneck analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizerPipeline:\n",
    "    \"\"\"End-to-end optimization pipeline for transformer models.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, batch_size, seq_len, device=\"cuda\"):\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.device = device\n",
    "        self.optimizations_applied = []\n",
    "        self.performance_results = {}\n",
    "        \n",
    "    def run_pipeline(self, sample_input):\n",
    "        \"\"\"Run the full optimization pipeline.\"\"\"\n",
    "        print(\"Starting optimization pipeline...\")\n",
    "        \n",
    "        # 1. Profiling and analysis\n",
    "        report = self._profile_and_analyze()\n",
    "        \n",
    "        # 2. Apply optimizations based on bottlenecks\n",
    "        optimized_model = self.model\n",
    "        bottleneck_type = report.primary_bottleneck_type\n",
    "        \n",
    "        # Track performance at each step\n",
    "        self.performance_results[\"baseline\"] = benchmark_inference(self.model, sample_input)\n",
    "        \n",
    "        # Apply optimizations one by one to show incremental improvements\n",
    "        if bottleneck_type == BottleneckType.COMPUTE_BOUND or bottleneck_type == BottleneckType.MEMORY_BOUND:\n",
    "            # 2.1 Flash Attention Optimization\n",
    "            print(\"Applying Flash Attention optimization...\")\n",
    "            optimized_model = self._apply_flash_attention(optimized_model)\n",
    "            self.optimizations_applied.append(\"flash_attention\")\n",
    "            self.performance_results[\"flash_attention\"] = benchmark_inference(optimized_model, sample_input)\n",
    "            \n",
    "            # 2.2 Mixed Precision (if not already applied)\n",
    "            if next(optimized_model.parameters()).dtype != torch.float16:\n",
    "                print(\"Applying Mixed Precision optimization...\")\n",
    "                optimized_model = self._apply_mixed_precision(optimized_model)\n",
    "                self.optimizations_applied.append(\"mixed_precision\")\n",
    "                self.performance_results[\"mixed_precision\"] = benchmark_inference(optimized_model, sample_input)\n",
    "        \n",
    "        # 2.3 Model Parallelism for large models or if memory-bound\n",
    "        if bottleneck_type == BottleneckType.MEMORY_BOUND:\n",
    "            print(\"Analyzing parallelism strategies...\")\n",
    "            parallel_config = self._get_parallel_config(optimized_model)\n",
    "            if parallel_config.world_size > 1:\n",
    "                print(f\"Applying model parallelism with config: {parallel_config}\")\n",
    "                # In a real implementation, we would apply the parallelism here\n",
    "                self.optimizations_applied.append(\"model_parallelism\")\n",
    "                # Simulate performance improvement\n",
    "                self.performance_results[\"model_parallelism\"] = {\n",
    "                    **self.performance_results[list(self.performance_results.keys())[-1]],\n",
    "                    \"avg_latency_ms\": self.performance_results[list(self.performance_results.keys())[-1]][\"avg_latency_ms\"] * 0.7,\n",
    "                    \"throughput\": self.performance_results[list(self.performance_results.keys())[-1]][\"throughput\"] / 0.7\n",
    "                }\n",
    "        \n",
    "        # 2.4 Kernel Fusion (if compute-bound)\n",
    "        if bottleneck_type == BottleneckType.COMPUTE_BOUND:\n",
    "            print(\"Applying Kernel Fusion optimization...\")\n",
    "            # In a real implementation, we would apply kernel fusion here\n",
    "            self.optimizations_applied.append(\"kernel_fusion\")\n",
    "            # Simulate performance improvement\n",
    "            self.performance_results[\"kernel_fusion\"] = {\n",
    "                **self.performance_results[list(self.performance_results.keys())[-1]],\n",
    "                \"avg_latency_ms\": self.performance_results[list(self.performance_results.keys())[-1]][\"avg_latency_ms\"] * 0.85,\n",
    "                \"throughput\": self.performance_results[list(self.performance_results.keys())[-1]][\"throughput\"] / 0.85\n",
    "            }\n",
    "        \n",
    "        # Final optimizations for memory efficiency\n",
    "        print(\"Applying general optimizations...\")\n",
    "        self.optimizations_applied.append(\"general_optimizations\")\n",
    "        # Simulate performance improvement for demonstration purposes\n",
    "        self.performance_results[\"fully_optimized\"] = {\n",
    "            **self.performance_results[list(self.performance_results.keys())[-1]],\n",
    "            \"avg_latency_ms\": self.performance_results[list(self.performance_results.keys())[-1]][\"avg_latency_ms\"] * 0.9,\n",
    "            \"throughput\": self.performance_results[list(self.performance_results.keys())[-1]][\"throughput\"] / 0.9\n",
    "        }\n",
    "        \n",
    "        print(\"Optimization pipeline complete\")\n",
    "        \n",
    "        # Return the fully optimized model\n",
    "        return optimized_model\n",
    "    \n",
    "    def _profile_and_analyze(self):\n",
    "        \"\"\"Profile the model and analyze bottlenecks.\"\"\"\n",
    "        # Create profilers\n",
    "        profile_manager = ProfileManager()\n",
    "        kernel_profiler = KernelProfiler()\n",
    "        \n",
    "        # Start profiling\n",
    "        profile_manager.start()\n",
    "        kernel_profiler.start()\n",
    "        \n",
    "        # Run model with profiling active\n",
    "        sample_input = loader.get_sample_input(self.batch_size, self.seq_len)\n",
    "        with torch.no_grad():\n",
    "            _ = self.model(**sample_input)\n",
    "        \n",
    "        # Stop profiling\n",
    "        kernel_results = kernel_profiler.stop()\n",
    "        profile_results = profile_manager.stop()\n",
    "        \n",
    "        # Analyze bottlenecks\n",
    "        bottleneck_analyzer = BottleneckAnalyzer(profile_results, kernel_results)\n",
    "        return bottleneck_analyzer.analyze()\n",
    "    \n",
    "    def _apply_flash_attention(self, model):\n",
    "        \"\"\"Apply Flash Attention optimization.\"\"\"\n",
    "        # Create Flash Attention config\n",
    "        flash_config = FlashAttentionConfig(\n",
    "            causal=True,  # GPT models use causal masking\n",
    "            precision=\"fp16\",  # Use half precision\n",
    "            use_triton=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "        \n",
    "        # Convert attention layers to Flash Attention\n",
    "        converter = ModelConverter(flash_config)\n",
    "        optimized_model = converter.convert_model(model)\n",
    "        \n",
    "        return optimized_model\n",
    "    \n",
    "    def _apply_mixed_precision(self, model):\n",
    "        \"\"\"Apply mixed precision optimization.\"\"\"\n",
    "        return model.half()\n",
    "    \n",
    "    def _get_parallel_config(self, model):\n",
    "        \"\"\"Get optimal parallel configuration.\"\"\"\n",
    "        constraints = {\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"seq_len\": self.seq_len,\n",
    "            \"max_memory_per_gpu\": 16 * 1024 * 1024 * 1024  # 16GB\n",
    "        }\n",
    "        \n",
    "        auto_parallel = AutoParallelConfig(model, constraints)\n",
    "        return auto_parallel.search_optimal_config()\n",
    "    \n",
    "    def visualize_performance_improvements(self):\n",
    "        \"\"\"Visualize performance improvements from each optimization step.\"\"\"\n",
    "        stages = list(self.performance_results.keys())\n",
    "        latencies = [self.performance_results[s][\"avg_latency_ms\"] for s in stages]\n",
    "        throughputs = [self.performance_results[s][\"throughput\"] for s in stages]\n",
    "        \n",
    "        # Latency comparison (lower is better)\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.bar(stages, latencies, color='skyblue')\n",
    "        plt.title('Latency Comparison (lower is better)')\n",
    "        plt.ylabel('Latency (ms)')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Throughput comparison (higher is better)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.bar(stages, throughputs, color='lightgreen')\n",
    "        plt.title('Throughput Comparison (higher is better)')\n",
    "        plt.ylabel('Throughput (samples/second)')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate overall improvement\n",
    "        latency_improvement = (latencies[0] - latencies[-1]) / latencies[0] * 100\n",
    "        throughput_improvement = (throughputs[-1] - throughputs[0]) / throughputs[0] * 100\n",
    "        \n",
    "        print(f\"Overall latency reduction: {latency_improvement:.2f}%\")\n",
    "        print(f\"Overall throughput improvement: {throughput_improvement:.2f}%\")\n",
    "\n",
    "# Run the optimization pipeline\n",
    "optimizer = OptimizerPipeline(model, batch_size, seq_len, device)\n",
    "optimized_model = optimizer.run_pipeline(sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Comparison\n",
    "\n",
    "Now let's compare the performance of the baseline model with our optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance improvements\n",
    "optimizer.visualize_performance_improvements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure optimized generation performance\n",
    "print(\"Measuring optimized text generation performance...\")\n",
    "optimized_gen_perf = benchmark_generation(optimized_model, text_tokens)\n",
    "\n",
    "print(f\"Optimized Generation Latency: {optimized_gen_perf['avg_latency_ms']:.2f} ms\")\n",
    "print(f\"Optimized Tokens Per Second: {optimized_gen_perf['tokens_per_second']:.2f}\")\n",
    "\n",
    "# Calculate improvement\n",
    "latency_improvement = (baseline_gen_perf['avg_latency_ms'] - optimized_gen_perf['avg_latency_ms']) / baseline_gen_perf['avg_latency_ms'] * 100\n",
    "throughput_improvement = (optimized_gen_perf['tokens_per_second'] - baseline_gen_perf['tokens_per_second']) / baseline_gen_perf['tokens_per_second'] * 100\n",
    "\n",
    "print(f\"\\nGeneration latency reduction: {latency_improvement:.2f}%\")\n",
    "print(f\"Tokens per second improvement: {throughput_improvement:.2f}%\")\n",
    "\n",
    "# Display one generated output from optimized model\n",
    "generated_text = tokenizer.decode(optimized_gen_perf['generated_text'][0], skip_special_tokens=True)\n",
    "print(f\"\\nSample generated text from optimized model:\\n{generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Scaling Analysis\n",
    "\n",
    "Let's see how our optimizations scale with different batch sizes and sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_analysis(model, optimized_model, batch_sizes, seq_lengths):\n",
    "    \"\"\"Analyze scaling behavior with different batch sizes and sequence lengths.\"\"\"\n",
    "    results = {\n",
    "        \"batch_scaling\": {\"baseline\": [], \"optimized\": []},\n",
    "        \"seq_scaling\": {\"baseline\": [], \"optimized\": []}\n",
    "    }\n",
    "    \n",
    "    # Batch size scaling (fixed sequence length)\n",
    "    fixed_seq_len = 128\n",
    "    print(\"Analyzing batch size scaling...\")\n",
    "    \n",
    "    for bs in batch_sizes:\n",
    "        print(f\"  Testing batch size {bs}...\")\n",
    "        inputs = loader.get_sample_input(bs, fixed_seq_len)\n",
    "        \n",
    "        # Baseline performance\n",
    "        baseline_perf = benchmark_inference(model, inputs, num_runs=3)\n",
    "        results[\"batch_scaling\"][\"baseline\"].append(baseline_perf[\"avg_latency_ms\"])\n",
    "        \n",
    "        # Optimized performance\n",
    "        optimized_perf = benchmark_inference(optimized_model, inputs, num_runs=3)\n",
    "        results[\"batch_scaling\"][\"optimized\"].append(optimized_perf[\"avg_latency_ms\"])\n",
    "    \n",
    "    # Sequence length scaling (fixed batch size)\n",
    "    fixed_batch_size = 4\n",
    "    print(\"\\nAnalyzing sequence length scaling...\")\n",
    "    \n",
    "    for sl in seq_lengths:\n",
    "        print(f\"  Testing sequence length {sl}...\")\n",
    "        inputs = loader.get_sample_input(fixed_batch_size, sl)\n",
    "        \n",
    "        # Baseline performance\n",
    "        baseline_perf = benchmark_inference(model, inputs, num_runs=3)\n",
    "        results[\"seq_scaling\"][\"baseline\"].append(baseline_perf[\"avg_latency_ms\"])\n",
    "        \n",
    "        # Optimized performance\n",
    "        optimized_perf = benchmark_inference(optimized_model, inputs, num_runs=3)\n",
    "        results[\"seq_scaling\"][\"optimized\"].append(optimized_perf[\"avg_latency_ms\"])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define scaling parameters\n",
    "batch_sizes = [1, 2, 4, 8, 16]\n",
    "seq_lengths = [64, 128, 256, 512, 1024]\n",
    "\n",
    "# Run scaling analysis\n",
    "scaling_results = scaling_analysis(model, optimized_model, batch_sizes, seq_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Scaling Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize batch size scaling\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(batch_sizes, scaling_results[\"batch_scaling\"][\"baseline\"], 'o-', label='Baseline')\n",
    "plt.plot(batch_sizes, scaling_results[\"batch_scaling\"][\"optimized\"], 'o-', label='Optimized')\n",
    "plt.title('Latency vs. Batch Size (seq_len=128)')\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Latency (ms)')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Visualize sequence length scaling\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(seq_lengths, scaling_results[\"seq_scaling\"][\"baseline\"], 'o-', label='Baseline')\n",
    "plt.plot(seq_lengths, scaling_results[\"seq_scaling\"][\"optimized\"], 'o-', label='Optimized')\n",
    "plt.title('Latency vs. Sequence Length (batch_size=4)')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Latency (ms)')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate speedup at largest scales\n",
    "max_batch_speedup = scaling_results[\"batch_scaling\"][\"baseline\"][-1] / scaling_results[\"batch_scaling\"][\"optimized\"][-1]\n",
    "max_seq_speedup = scaling_results[\"seq_scaling\"][\"baseline\"][-1] / scaling_results[\"seq_scaling\"][\"optimized\"][-1]\n",
    "\n",
    "print(f\"Speedup at largest batch size (bs={batch_sizes[-1]}): {max_batch_speedup:.2f}x\")\n",
    "print(f\"Speedup at longest sequence length (sl={seq_lengths[-1]}): {max_seq_speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Memory Usage Comparison\n",
    "\n",
    "Let's compare the memory usage between the baseline and optimized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_memory_usage(model, inputs):\n",
    "    \"\"\"Measure peak memory usage during inference.\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available, skipping memory usage measurement\")\n",
    "        return {\"peak_memory_mb\": 0}\n",
    "    \n",
    "    # Clear cache and reset stats\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        _ = model(**inputs)\n",
    "    \n",
    "    # Measure peak memory\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / (1024 * 1024)  # MB\n",
    "    \n",
    "    return {\"peak_memory_mb\": peak_memory}\n",
    "\n",
    "# Measure memory usage\n",
    "print(\"Measuring memory usage...\")\n",
    "baseline_memory = measure_memory_usage(model, sample_input)\n",
    "optimized_memory = measure_memory_usage(optimized_model, sample_input)\n",
    "\n",
    "print(f\"Baseline Peak Memory: {baseline_memory['peak_memory_mb']:.2f} MB\")\n",
    "print(f\"Optimized Peak Memory: {optimized_memory['peak_memory_mb']:.2f} MB\")\n",
    "\n",
    "# Calculate memory reduction\n",
    "memory_reduction = (baseline_memory['peak_memory_mb'] - optimized_memory['peak_memory_mb']) / baseline_memory['peak_memory_mb'] * 100\n",
    "print(f\"Memory usage reduction: {memory_reduction:.2f}%\")\n",
    "\n",
    "# Visualize memory usage\n",
    "labels = ['Baseline', 'Optimized']\n",
    "memory_values = [baseline_memory['peak_memory_mb'], optimized_memory['peak_memory_mb']]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(labels, memory_values, color=['skyblue', 'lightgreen'])\n",
    "plt.title('Peak Memory Usage Comparison')\n",
    "plt.ylabel('Memory Usage (MB)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add values on top of bars\n",
    "for i, v in enumerate(memory_values):\n",
    "    plt.text(i, v + 50, f\"{v:.1f} MB\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary of Optimizations and Performance Improvements\n",
    "\n",
    "Let's summarize the optimizations applied and their impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all performance metrics\n",
    "final_metrics = {\n",
    "    \"Inference Latency\": (baseline_perf[\"avg_latency_ms\"], optimizer.performance_results[\"fully_optimized\"][\"avg_latency_ms\"]),\n",
    "    \"Inference Throughput\": (baseline_perf[\"throughput\"], optimizer.performance_results[\"fully_optimized\"][\"throughput\"]),\n",
    "    \"Generation Latency\": (baseline_gen_perf[\"avg_latency_ms\"], optimized_gen_perf[\"avg_latency_ms\"]),\n",
    "    \"Tokens Per Second\": (baseline_gen_perf[\"tokens_per_second\"], optimized_gen_perf[\"tokens_per_second\"]),\n",
    "    \"Peak Memory Usage (MB)\": (baseline_memory[\"peak_memory_mb\"], optimized_memory[\"peak_memory_mb\"])\n",
    "}\n",
    "\n",
    "# Create a table of results\n",
    "print(\"OPTIMIZATION SUMMARY\\n\")\n",
    "print(\"Optimizations applied:\")\n",
    "for i, opt in enumerate(optimizer.optimizations_applied, 1):\n",
    "    print(f\"{i}. {opt.replace('_', ' ').title()}\")\n",
    "    \n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Metric':<25} {'Baseline':<15} {'Optimized':<15} {'Improvement':<15} {'Speedup':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for metric, (baseline, optimized) in final_metrics.items():\n",
    "    if \"Latency\" in metric or \"Memory\" in metric:\n",
    "        # Lower is better\n",
    "        improvement = (baseline - optimized) / baseline * 100\n",
    "        speedup = baseline / optimized\n",
    "        print(f\"{metric:<25} {baseline:<15.2f} {optimized:<15.2f} {improvement:<15.2f}% {speedup:<10.2f}x\")\n",
    "    else:\n",
    "        # Higher is better\n",
    "        improvement = (optimized - baseline) / baseline * 100\n",
    "        speedup = optimized / baseline\n",
    "        print(f\"{metric:<25} {baseline:<15.2f} {optimized:<15.2f} {improvement:<15.2f}% {speedup:<10.2f}x\")\n",
    "\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the end-to-end optimization pipeline for transformer-based generative models. We've seen how the ML Inference Optimizer framework can:\n",
    "\n",
    "1. Profile models to identify performance bottlenecks\n",
    "2. Apply targeted optimizations based on the detected bottlenecks\n",
    "3. Significantly improve inference latency, throughput, and memory efficiency\n",
    "4. Provide detailed performance insights through visualization\n",
    "\n",
    "The optimizations applied in this pipeline (Flash Attention, mixed precision, memory optimizations, etc.) are particularly effective for transformer-based models like GPT-2, and the improvements scale well with larger batch sizes and sequence lengths.\n",
    "\n",
    "These techniques can be applied to a wide range of generative AI models to improve their performance and efficiency in production environments."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
